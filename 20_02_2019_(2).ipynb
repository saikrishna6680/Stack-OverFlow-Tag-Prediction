{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20-02-2019 (2).ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "v9KlB8bx1QrL",
        "colab_type": "code",
        "outputId": "56dadb7d-16b8-474c-e5c5-cc00a7d85da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!curl --header \"Host: storage.googleapis.com\" --header \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36\" --header \"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\" --header \"Accept-Language: en-US,en;q=0.9,zu;q=0.8,ca;q=0.7,sn;q=0.6,jv;q=0.5,fi;q=0.4,te;q=0.3\" --header \"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle/3539/Train.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1553505736&Signature=eeiFoPYsa6QD3S7mFk2L4Z8%2FliFaOFC5mdZ4gdasSViBtn2ONS1E%2FvB%2FnR0I9ejTlze6ZiE%2FFuhw6MQuMNIQgMrNT%2FeEg7EdE0mzKYIpppZTat3n19ihtMbDzWaiMuv48p9edcUf78TkCrl%2FxpjV6h68TI%2Bz%2BPj9hfmaRFJcdIQIQ0tjKshu0qgyTcw4P02G2q8PE6IUlddhcvr%2BpeJj1qRXtio74XurNKaIHv4vmdV%2Ba0Q4oW9EaUsImJQzPQ9F0rSsEPYBOjsFzPOJx8Y6tcWTkXyreNbOwYMLHHMlXfYth7hXX0lGgoD8ahpHaXUD7lgbb3Vd9WP%2FqPFgFlEo5w%3D%3D\" -o \"Train.zip\" -L"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2238M  100 2238M    0     0  83.8M      0  0:00:26  0:00:26 --:--:--  105M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2vhG8kWq1O1I",
        "colab_type": "code",
        "outputId": "85a39a80-82c1-4169-896a-cfa78f134732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip Train.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  Train.zip\n",
            "  inflating: Train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1ShbZZTN1bea",
        "colab_type": "code",
        "outputId": "bb0f641d-10a6-497a-bfda-b8a51aad399c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!curl --header \"Host: storage.googleapis.com\" --header \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36\" --header \"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\" --header \"Accept-Language: en-US,en;q=0.9,zu;q=0.8,ca;q=0.7,sn;q=0.6,jv;q=0.5,fi;q=0.4,te;q=0.3\" --header \"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle/3539/Test.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1553505701&Signature=JHnbVRtdplvtudJYZwaKt8QbAJPso%2Fn1IR6qxyCmzO91z1t3QOxEZODRHNKpnAr8c3NOAQ3INtaViksd7JD6OJmSm38elvFNjAdMF6IVp%2FLxrCgPmTQiHq1Q3URKxDmZnpKbIryjCHj7h%2FwsUyEAPHVIUdcHfsbQcTDaSwCu29vHoqw%2FcUieNugIN6PuHysk1%2FfJ%2BIIZGPb0bGVQDea1k9HiU2EqVSoK3Z0UnMOAbUzEWv1muk%2BiLtLb4dz3pc%2Fg1RALIRxn89xC8MiSUoPptP14oBazrEtginRpJt3%2BvbW7Af%2FCVFBIQi3JIEUCW13O34Vobkz%2FBSXgpA0BG234dw%3D%3D\" -o \"Test.zip\" -L\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  725M  100  725M    0     0   109M      0  0:00:06  0:00:06 --:--:--  114M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3PvrXN4F1bx-",
        "colab_type": "code",
        "outputId": "6a5d22fd-84cb-447c-b2f5-3a3da9a4d671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip Test.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  Test.zip\n",
            "  inflating: Test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "U5Plrr2NU93P"
      },
      "cell_type": "markdown",
      "source": [
        "#  STACK OVERFLOW ASSIGNMENT"
      ]
    },
    {
      "metadata": {
        "id": "Hbb6aSaj3Lmn",
        "colab_type": "code",
        "outputId": "f38a866c-4276-4363-d87f-2b15d9146ba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install scikit-multilearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-multilearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/1f/e6ff649c72a1cdf2c7a1d31eb21705110ce1c5d3e7e26b2cc300e1637272/scikit_multilearn-0.2.0-py3-none-any.whl (89kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 4.2MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ihs1Hb3RU93S",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import os\n",
        "from sqlalchemy import create_engine # database connection\n",
        "import datetime as dt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from skmultilearn.adapt import mlknn\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from datetime import datetime\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rAZDkLNPU93Y"
      },
      "cell_type": "markdown",
      "source": [
        "# Stack Overflow: Tag Prediction"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TpVk_usWU93a"
      },
      "cell_type": "markdown",
      "source": [
        "<h1>1. Business Problem </h1>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5-1WlUboU93c"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 1.1 Description </h2>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "37SdsKLdU93e"
      },
      "cell_type": "markdown",
      "source": [
        "<p style='font-size:18px'><b> Description </b></p>\n",
        "<p>\n",
        "Stack Overflow is the largest, most trusted online community for developers to learn, share their programming knowledge, and build their careers.<br />\n",
        "<br />\n",
        "Stack Overflow is something which every programmer use one way or another. Each month, over 50 million developers come to Stack Overflow to learn, share their knowledge, and build their careers. It features questions and answers on a wide range of topics in computer programming. The website serves as a platform for users to ask and answer questions, and, through membership and active participation, to vote questions and answers up or down and edit questions and answers in a fashion similar to a wiki or Digg. As of April 2014 Stack Overflow has over 4,000,000 registered users, and it exceeded 10,000,000 questions in late August 2015. Based on the type of tags assigned to questions, the top eight most discussed topics on the site are: Java, JavaScript, C#, PHP, Android, jQuery, Python and HTML.<br />\n",
        "<br />\n",
        "</p>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9brAMjUsU93f"
      },
      "cell_type": "markdown",
      "source": [
        "<p style='font-size:18px'><b> Problem Statemtent </b></p>\n",
        "Suggest the tags based on the content that was there in the question posted on Stackoverflow."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "URJINSY2U93h"
      },
      "cell_type": "markdown",
      "source": [
        "<p style='font-size:18px'><b> Source:  </b> https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/</p>\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ggC_3T9XU93j"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 1.2 Source / useful links </h2>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Nr7T9iknU93l"
      },
      "cell_type": "markdown",
      "source": [
        "Data Source : https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/data <br>\n",
        "Youtube : https://youtu.be/nNDqbUhtIRg <br>\n",
        "Research paper : https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tagging-1.pdf <br>\n",
        "Research paper : https://dl.acm.org/citation.cfm?id=2660970&dl=ACM&coll=DL"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hbXm8lAuU93p"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 1.3 Real World / Business Objectives and Constraints </h2>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "88TwKPItU93q"
      },
      "cell_type": "markdown",
      "source": [
        "1. Predict as many tags as possible with high precision and recall.\n",
        "2. Incorrect tags could impact customer experience on StackOverflow.\n",
        "3. No strict latency constraints."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lE0wX1roU93s"
      },
      "cell_type": "markdown",
      "source": [
        "<h1>2. Machine Learning problem </h1>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ynAszp_uU93u"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 2.1 Data </h2>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "elwijxMGU93w"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 2.1.1 Data Overview </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mdFiIj7_U93x"
      },
      "cell_type": "markdown",
      "source": [
        "Refer: https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/data\n",
        "<br>\n",
        "All of the data is in 2 files: Train and Test.<br />\n",
        "<pre>\n",
        "<b>Train.csv</b> contains 4 columns: Id,Title,Body,Tags.<br />\n",
        "<b>Test.csv</b> contains the same columns but without the Tags, which you are to predict.<br />\n",
        "<b>Size of Train.csv</b> - 6.75GB<br />\n",
        "<b>Size of Test.csv</b> - 2GB<br />\n",
        "<b>Number of rows in Train.csv</b> = 6034195<br />\n",
        "</pre>\n",
        "The questions are randomized and contains a mix of verbose text sites as well as sites related to math and programming. The number of questions from each site may vary, and no filtering has been performed on the questions (such as closed questions).<br />\n",
        "<br />\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ji0A66hWU93z"
      },
      "cell_type": "markdown",
      "source": [
        "__Data Field Explaination__\n",
        "\n",
        "Dataset contains 6,034,195 rows. The columns in the table are:<br />\n",
        "<pre>\n",
        "<b>Id</b> - Unique identifier for each question<br />\n",
        "<b>Title</b> - The question's title<br />\n",
        "<b>Body</b> - The body of the question<br />\n",
        "<b>Tags</b> - The tags associated with the question in a space-seperated format (all lowercase, should not contain tabs '\\t' or ampersands '&')<br />\n",
        "</pre>\n",
        "\n",
        "<br />"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WNDiy42GU931"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>2.1.2 Example Data point </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "D5IcxRbYU932"
      },
      "cell_type": "markdown",
      "source": [
        "<pre>\n",
        "<b>Title</b>:  Implementing Boundary Value Analysis of Software Testing in a C++ program?\n",
        "<b>Body </b>: <pre><code>\n",
        "        #include&lt;\n",
        "        iostream&gt;\\n\n",
        "        #include&lt;\n",
        "        stdlib.h&gt;\\n\\n\n",
        "        using namespace std;\\n\\n\n",
        "        int main()\\n\n",
        "        {\\n\n",
        "                 int n,a[n],x,c,u[n],m[n],e[n][4];\\n         \n",
        "                 cout&lt;&lt;\"Enter the number of variables\";\\n         cin&gt;&gt;n;\\n\\n         \n",
        "                 cout&lt;&lt;\"Enter the Lower, and Upper Limits of the variables\";\\n         \n",
        "                 for(int y=1; y&lt;n+1; y++)\\n         \n",
        "                 {\\n                 \n",
        "                    cin&gt;&gt;m[y];\\n                 \n",
        "                    cin&gt;&gt;u[y];\\n         \n",
        "                 }\\n         \n",
        "                 for(x=1; x&lt;n+1; x++)\\n         \n",
        "                 {\\n                 \n",
        "                    a[x] = (m[x] + u[x])/2;\\n         \n",
        "                 }\\n         \n",
        "                 c=(n*4)-4;\\n         \n",
        "                 for(int a1=1; a1&lt;n+1; a1++)\\n         \n",
        "                 {\\n\\n             \n",
        "                    e[a1][0] = m[a1];\\n             \n",
        "                    e[a1][1] = m[a1]+1;\\n             \n",
        "                    e[a1][2] = u[a1]-1;\\n             \n",
        "                    e[a1][3] = u[a1];\\n         \n",
        "                 }\\n         \n",
        "                 for(int i=1; i&lt;n+1; i++)\\n         \n",
        "                 {\\n            \n",
        "                    for(int l=1; l&lt;=i; l++)\\n            \n",
        "                    {\\n                 \n",
        "                        if(l!=1)\\n                 \n",
        "                        {\\n                    \n",
        "                            cout&lt;&lt;a[l]&lt;&lt;\"\\\\t\";\\n                 \n",
        "                        }\\n            \n",
        "                    }\\n            \n",
        "                    for(int j=0; j&lt;4; j++)\\n            \n",
        "                    {\\n                \n",
        "                        cout&lt;&lt;e[i][j];\\n                \n",
        "                        for(int k=0; k&lt;n-(i+1); k++)\\n                \n",
        "                        {\\n                    \n",
        "                            cout&lt;&lt;a[k]&lt;&lt;\"\\\\t\";\\n               \n",
        "                        }\\n                \n",
        "                        cout&lt;&lt;\"\\\\n\";\\n            \n",
        "                    }\\n        \n",
        "                 }    \\n\\n        \n",
        "                 system(\"PAUSE\");\\n        \n",
        "                 return 0;    \\n\n",
        "        }\\n\n",
        "        </code></pre>\\n\\n\n",
        "        <p>The answer should come in the form of a table like</p>\\n\\n\n",
        "        <pre><code>       \n",
        "        1            50              50\\n       \n",
        "        2            50              50\\n       \n",
        "        99           50              50\\n       \n",
        "        100          50              50\\n       \n",
        "        50           1               50\\n       \n",
        "        50           2               50\\n       \n",
        "        50           99              50\\n       \n",
        "        50           100             50\\n       \n",
        "        50           50              1\\n       \n",
        "        50           50              2\\n       \n",
        "        50           50              99\\n       \n",
        "        50           50              100\\n\n",
        "        </code></pre>\\n\\n\n",
        "        <p>if the no of inputs is 3 and their ranges are\\n\n",
        "        1,100\\n\n",
        "        1,100\\n\n",
        "        1,100\\n\n",
        "        (could be varied too)</p>\\n\\n\n",
        "        <p>The output is not coming,can anyone correct the code or tell me what\\'s wrong?</p>\\n'\n",
        "<b>Tags </b>: 'c++ c'\n",
        "</pre>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MomJjFPnU934"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>2.2 Mapping the real-world problem to a Machine Learning Problem </h2>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sJ5lyxIUU936"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 2.2.1 Type of Machine Learning Problem </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5Jb9u-38U938"
      },
      "cell_type": "markdown",
      "source": [
        "<p> It is a multi-label classification problem  <br>\n",
        "<b>Multi-label Classification</b>: Multilabel classification assigns to each sample a set of target labels. This can be thought as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A question on Stackoverflow might be about any of C, Pointers, FileIO and/or memory-management at the same time or none of these. <br>\n",
        "__Credit__: http://scikit-learn.org/stable/modules/multiclass.html\n",
        "</p>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QIOwycKkU93_"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>2.2.2 Performance metric </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DDQXZ4k5U94A"
      },
      "cell_type": "markdown",
      "source": [
        "<b>Micro-Averaged F1-Score (Mean F Score) </b>: \n",
        "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
        "\n",
        "<i>F1 = 2 * (precision * recall) / (precision + recall)</i><br>\n",
        "\n",
        "In the multi-class and multi-label case, this is the weighted average of the F1 score of each class. <br>\n",
        "\n",
        "<b>'Micro f1 score': </b><br>\n",
        "Calculate metrics globally by counting the total true positives, false negatives and false positives. This is a better metric when we have class imbalance.\n",
        "<br>\n",
        "\n",
        "<b>'Macro f1 score': </b><br>\n",
        "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
        "<br>\n",
        "\n",
        "https://www.kaggle.com/wiki/MeanFScore <br>\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html <br>\n",
        "<br>\n",
        "<b> Hamming loss </b>: The Hamming loss is the fraction of labels that are incorrectly predicted. <br>\n",
        "https://www.kaggle.com/wiki/HammingLoss <br>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xqsVRjSmU94C"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> 3. Exploratory Data Analysis </h1>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "azda-BmRU94H"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 3.1 Data Loading and Cleaning </h2>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "62GDC_VjU94J"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>3.1.1 Using Pandas with SQLite to Load the data</h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "289tS71cU94L",
        "outputId": "b569bdbc-776c-4552-dd65-8b8eac4fd654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5746
        }
      },
      "cell_type": "code",
      "source": [
        "#checking file is present or not\n",
        "if not os.path.isfile('train.db'):\n",
        "    #calculating time how time it has been taken for running this cell\n",
        "    start = datetime.now()\n",
        "    #creating sql database file \n",
        "    disk_engine = create_engine('sqlite:///train.db')\n",
        "    start = dt.datetime.now()\n",
        "    #instead of loading total file i am chunks to loads\n",
        "    chunksize = 18000\n",
        "    j = 0\n",
        "    index_start = 1\n",
        "    #Reading csv file\n",
        "    for df in pd.read_csv('Train.csv', names=['Id', 'Title', 'Body', 'Tags'], chunksize=chunksize, iterator=True, encoding='utf-8', ):\n",
        "        df.index += index_start\n",
        "        j+=1\n",
        "        #printing rows in csv file\n",
        "        print('{} rows'.format(j*chunksize))\n",
        "        df.to_sql('data', disk_engine, if_exists='append')      \n",
        "        index_start = df.index[-1] + 1\n",
        "    print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18000 rows\n",
            "36000 rows\n",
            "54000 rows\n",
            "72000 rows\n",
            "90000 rows\n",
            "108000 rows\n",
            "126000 rows\n",
            "144000 rows\n",
            "162000 rows\n",
            "180000 rows\n",
            "198000 rows\n",
            "216000 rows\n",
            "234000 rows\n",
            "252000 rows\n",
            "270000 rows\n",
            "288000 rows\n",
            "306000 rows\n",
            "324000 rows\n",
            "342000 rows\n",
            "360000 rows\n",
            "378000 rows\n",
            "396000 rows\n",
            "414000 rows\n",
            "432000 rows\n",
            "450000 rows\n",
            "468000 rows\n",
            "486000 rows\n",
            "504000 rows\n",
            "522000 rows\n",
            "540000 rows\n",
            "558000 rows\n",
            "576000 rows\n",
            "594000 rows\n",
            "612000 rows\n",
            "630000 rows\n",
            "648000 rows\n",
            "666000 rows\n",
            "684000 rows\n",
            "702000 rows\n",
            "720000 rows\n",
            "738000 rows\n",
            "756000 rows\n",
            "774000 rows\n",
            "792000 rows\n",
            "810000 rows\n",
            "828000 rows\n",
            "846000 rows\n",
            "864000 rows\n",
            "882000 rows\n",
            "900000 rows\n",
            "918000 rows\n",
            "936000 rows\n",
            "954000 rows\n",
            "972000 rows\n",
            "990000 rows\n",
            "1008000 rows\n",
            "1026000 rows\n",
            "1044000 rows\n",
            "1062000 rows\n",
            "1080000 rows\n",
            "1098000 rows\n",
            "1116000 rows\n",
            "1134000 rows\n",
            "1152000 rows\n",
            "1170000 rows\n",
            "1188000 rows\n",
            "1206000 rows\n",
            "1224000 rows\n",
            "1242000 rows\n",
            "1260000 rows\n",
            "1278000 rows\n",
            "1296000 rows\n",
            "1314000 rows\n",
            "1332000 rows\n",
            "1350000 rows\n",
            "1368000 rows\n",
            "1386000 rows\n",
            "1404000 rows\n",
            "1422000 rows\n",
            "1440000 rows\n",
            "1458000 rows\n",
            "1476000 rows\n",
            "1494000 rows\n",
            "1512000 rows\n",
            "1530000 rows\n",
            "1548000 rows\n",
            "1566000 rows\n",
            "1584000 rows\n",
            "1602000 rows\n",
            "1620000 rows\n",
            "1638000 rows\n",
            "1656000 rows\n",
            "1674000 rows\n",
            "1692000 rows\n",
            "1710000 rows\n",
            "1728000 rows\n",
            "1746000 rows\n",
            "1764000 rows\n",
            "1782000 rows\n",
            "1800000 rows\n",
            "1818000 rows\n",
            "1836000 rows\n",
            "1854000 rows\n",
            "1872000 rows\n",
            "1890000 rows\n",
            "1908000 rows\n",
            "1926000 rows\n",
            "1944000 rows\n",
            "1962000 rows\n",
            "1980000 rows\n",
            "1998000 rows\n",
            "2016000 rows\n",
            "2034000 rows\n",
            "2052000 rows\n",
            "2070000 rows\n",
            "2088000 rows\n",
            "2106000 rows\n",
            "2124000 rows\n",
            "2142000 rows\n",
            "2160000 rows\n",
            "2178000 rows\n",
            "2196000 rows\n",
            "2214000 rows\n",
            "2232000 rows\n",
            "2250000 rows\n",
            "2268000 rows\n",
            "2286000 rows\n",
            "2304000 rows\n",
            "2322000 rows\n",
            "2340000 rows\n",
            "2358000 rows\n",
            "2376000 rows\n",
            "2394000 rows\n",
            "2412000 rows\n",
            "2430000 rows\n",
            "2448000 rows\n",
            "2466000 rows\n",
            "2484000 rows\n",
            "2502000 rows\n",
            "2520000 rows\n",
            "2538000 rows\n",
            "2556000 rows\n",
            "2574000 rows\n",
            "2592000 rows\n",
            "2610000 rows\n",
            "2628000 rows\n",
            "2646000 rows\n",
            "2664000 rows\n",
            "2682000 rows\n",
            "2700000 rows\n",
            "2718000 rows\n",
            "2736000 rows\n",
            "2754000 rows\n",
            "2772000 rows\n",
            "2790000 rows\n",
            "2808000 rows\n",
            "2826000 rows\n",
            "2844000 rows\n",
            "2862000 rows\n",
            "2880000 rows\n",
            "2898000 rows\n",
            "2916000 rows\n",
            "2934000 rows\n",
            "2952000 rows\n",
            "2970000 rows\n",
            "2988000 rows\n",
            "3006000 rows\n",
            "3024000 rows\n",
            "3042000 rows\n",
            "3060000 rows\n",
            "3078000 rows\n",
            "3096000 rows\n",
            "3114000 rows\n",
            "3132000 rows\n",
            "3150000 rows\n",
            "3168000 rows\n",
            "3186000 rows\n",
            "3204000 rows\n",
            "3222000 rows\n",
            "3240000 rows\n",
            "3258000 rows\n",
            "3276000 rows\n",
            "3294000 rows\n",
            "3312000 rows\n",
            "3330000 rows\n",
            "3348000 rows\n",
            "3366000 rows\n",
            "3384000 rows\n",
            "3402000 rows\n",
            "3420000 rows\n",
            "3438000 rows\n",
            "3456000 rows\n",
            "3474000 rows\n",
            "3492000 rows\n",
            "3510000 rows\n",
            "3528000 rows\n",
            "3546000 rows\n",
            "3564000 rows\n",
            "3582000 rows\n",
            "3600000 rows\n",
            "3618000 rows\n",
            "3636000 rows\n",
            "3654000 rows\n",
            "3672000 rows\n",
            "3690000 rows\n",
            "3708000 rows\n",
            "3726000 rows\n",
            "3744000 rows\n",
            "3762000 rows\n",
            "3780000 rows\n",
            "3798000 rows\n",
            "3816000 rows\n",
            "3834000 rows\n",
            "3852000 rows\n",
            "3870000 rows\n",
            "3888000 rows\n",
            "3906000 rows\n",
            "3924000 rows\n",
            "3942000 rows\n",
            "3960000 rows\n",
            "3978000 rows\n",
            "3996000 rows\n",
            "4014000 rows\n",
            "4032000 rows\n",
            "4050000 rows\n",
            "4068000 rows\n",
            "4086000 rows\n",
            "4104000 rows\n",
            "4122000 rows\n",
            "4140000 rows\n",
            "4158000 rows\n",
            "4176000 rows\n",
            "4194000 rows\n",
            "4212000 rows\n",
            "4230000 rows\n",
            "4248000 rows\n",
            "4266000 rows\n",
            "4284000 rows\n",
            "4302000 rows\n",
            "4320000 rows\n",
            "4338000 rows\n",
            "4356000 rows\n",
            "4374000 rows\n",
            "4392000 rows\n",
            "4410000 rows\n",
            "4428000 rows\n",
            "4446000 rows\n",
            "4464000 rows\n",
            "4482000 rows\n",
            "4500000 rows\n",
            "4518000 rows\n",
            "4536000 rows\n",
            "4554000 rows\n",
            "4572000 rows\n",
            "4590000 rows\n",
            "4608000 rows\n",
            "4626000 rows\n",
            "4644000 rows\n",
            "4662000 rows\n",
            "4680000 rows\n",
            "4698000 rows\n",
            "4716000 rows\n",
            "4734000 rows\n",
            "4752000 rows\n",
            "4770000 rows\n",
            "4788000 rows\n",
            "4806000 rows\n",
            "4824000 rows\n",
            "4842000 rows\n",
            "4860000 rows\n",
            "4878000 rows\n",
            "4896000 rows\n",
            "4914000 rows\n",
            "4932000 rows\n",
            "4950000 rows\n",
            "4968000 rows\n",
            "4986000 rows\n",
            "5004000 rows\n",
            "5022000 rows\n",
            "5040000 rows\n",
            "5058000 rows\n",
            "5076000 rows\n",
            "5094000 rows\n",
            "5112000 rows\n",
            "5130000 rows\n",
            "5148000 rows\n",
            "5166000 rows\n",
            "5184000 rows\n",
            "5202000 rows\n",
            "5220000 rows\n",
            "5238000 rows\n",
            "5256000 rows\n",
            "5274000 rows\n",
            "5292000 rows\n",
            "5310000 rows\n",
            "5328000 rows\n",
            "5346000 rows\n",
            "5364000 rows\n",
            "5382000 rows\n",
            "5400000 rows\n",
            "5418000 rows\n",
            "5436000 rows\n",
            "5454000 rows\n",
            "5472000 rows\n",
            "5490000 rows\n",
            "5508000 rows\n",
            "5526000 rows\n",
            "5544000 rows\n",
            "5562000 rows\n",
            "5580000 rows\n",
            "5598000 rows\n",
            "5616000 rows\n",
            "5634000 rows\n",
            "5652000 rows\n",
            "5670000 rows\n",
            "5688000 rows\n",
            "5706000 rows\n",
            "5724000 rows\n",
            "5742000 rows\n",
            "5760000 rows\n",
            "5778000 rows\n",
            "5796000 rows\n",
            "5814000 rows\n",
            "5832000 rows\n",
            "5850000 rows\n",
            "5868000 rows\n",
            "5886000 rows\n",
            "5904000 rows\n",
            "5922000 rows\n",
            "5940000 rows\n",
            "5958000 rows\n",
            "5976000 rows\n",
            "5994000 rows\n",
            "6012000 rows\n",
            "6030000 rows\n",
            "6048000 rows\n",
            "Time taken to run this cell : 0:04:35.536047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "d5yUhXVNU94Q"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 3.1.2 Counting the number of rows </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3ORCclXYU94R",
        "outputId": "2cb19324-5adf-4753-848a-3d8fa9979c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#checking file is present or not\n",
        "if os.path.isfile('train.db'):\n",
        "    start = datetime.now()\n",
        "    #creating sql file\n",
        "    con = sqlite3.connect('train.db')\n",
        "    num_rows = pd.read_sql_query(\"\"\"SELECT count(*) FROM data\"\"\", con)\n",
        "    #Always remember to close the database\n",
        "    print(\"Number of rows in the database :\",\"\\n\",num_rows['count(*)'].values[0])\n",
        "    con.close()\n",
        "    print(\"Time taken to count the number of rows :\", datetime.now() - start)\n",
        "else:\n",
        "    print(\"Please download the train.db file from drive or run the above cell to genarate train.db file\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows in the database : \n",
            " 6034196\n",
            "Time taken to count the number of rows : 0:00:09.736591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Xso2eOEvU94Z"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>3.1.3 Checking for duplicates </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iBHCcr3DU94b",
        "outputId": "51665ac6-619a-4887-c84a-50e141a2fa1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Learn SQl: https://www.w3schools.com/sql/default.asp\n",
        "#checking file is present or not\n",
        "if os.path.isfile('train.db'):\n",
        "    start = datetime.now()\n",
        "    #creating sql sql file to store data\n",
        "    con = sqlite3.connect('train.db')\n",
        "    df_no_dup = pd.read_sql_query('SELECT Title, Body, Tags, COUNT(*) as cnt_dup FROM data GROUP BY Title, Body, Tags', con)\n",
        "    con.close()\n",
        "    print(\"Time taken to run this cell :\", datetime.now() - start)\n",
        "else:\n",
        "    print(\"Please download the train.db file from drive or run the first to genarate train.db file\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to run this cell : 0:03:02.864929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Gap4NRPWU94h",
        "outputId": "92ad440d-93a9-470a-c846-dcc4afd3c095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "#printing top 5 rows \n",
        "df_no_dup.head()\n",
        "# we can observe that there are duplicates"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Body</th>\n",
              "      <th>Tags</th>\n",
              "      <th>cnt_dup</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Implementing Boundary Value Analysis of S...</td>\n",
              "      <td>&lt;pre&gt;&lt;code&gt;#include&amp;lt;iostream&amp;gt;\\n#include&amp;...</td>\n",
              "      <td>c++ c</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dynamic Datagrid Binding in Silverlight?</td>\n",
              "      <td>&lt;p&gt;I should do binding for datagrid dynamicall...</td>\n",
              "      <td>c# silverlight data-binding</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dynamic Datagrid Binding in Silverlight?</td>\n",
              "      <td>&lt;p&gt;I should do binding for datagrid dynamicall...</td>\n",
              "      <td>c# silverlight data-binding columns</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>java.lang.NoClassDefFoundError: javax/serv...</td>\n",
              "      <td>&lt;p&gt;I followed the guide in &lt;a href=\"http://sta...</td>\n",
              "      <td>jsp jstl</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>java.sql.SQLException:[Microsoft][ODBC Dri...</td>\n",
              "      <td>&lt;p&gt;I use the following code&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;...</td>\n",
              "      <td>java jdbc</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Title  \\\n",
              "0       Implementing Boundary Value Analysis of S...   \n",
              "1           Dynamic Datagrid Binding in Silverlight?   \n",
              "2           Dynamic Datagrid Binding in Silverlight?   \n",
              "3      java.lang.NoClassDefFoundError: javax/serv...   \n",
              "4      java.sql.SQLException:[Microsoft][ODBC Dri...   \n",
              "\n",
              "                                                Body  \\\n",
              "0  <pre><code>#include&lt;iostream&gt;\\n#include&...   \n",
              "1  <p>I should do binding for datagrid dynamicall...   \n",
              "2  <p>I should do binding for datagrid dynamicall...   \n",
              "3  <p>I followed the guide in <a href=\"http://sta...   \n",
              "4  <p>I use the following code</p>\\n\\n<pre><code>...   \n",
              "\n",
              "                                  Tags  cnt_dup  \n",
              "0                                c++ c        1  \n",
              "1          c# silverlight data-binding        1  \n",
              "2  c# silverlight data-binding columns        1  \n",
              "3                             jsp jstl        1  \n",
              "4                            java jdbc        2  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JzFO4EeDU94n",
        "outputId": "e01bac2a-7a83-41c5-e867-4e86bef2f329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"number of duplicate questions :\", num_rows['count(*)'].values[0]- df_no_dup.shape[0], \"(\",(1-((df_no_dup.shape[0])/(num_rows['count(*)'].values[0])))*100,\"% )\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of duplicate questions : 1827881 ( 30.292038906260256 % )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gd2VdpN6U94t",
        "outputId": "1e79217f-d2da-4da8-ebd2-2b9b2e068354",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "# number of times each question appeared in our database\n",
        "df_no_dup.cnt_dup.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    2656284\n",
              "2    1272336\n",
              "3     277575\n",
              "4         90\n",
              "5         25\n",
              "6          5\n",
              "Name: cnt_dup, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "sCB4NKZU1J5O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Checking null values present or not\n",
        "df_no_dup[\"Tags\"].isnull().sum()\n",
        "data=df_no_dup[\"Tags\"].to_frame()\n",
        "sns.heatmap(data.isnull(),cbar=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xjKOQPtx1J5t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Checking null values at which row there is null value?\n",
        "df_no_dup[df_no_dup[\"Tags\"].isnull()]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3uiLHQ3m1J57",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_no_dup = df_no_dup.dropna(axis = 0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pHxcp-0w1J6E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start = datetime.now()\n",
        "df_no_dup[\"tag_count\"] = df_no_dup[\"Tags\"].apply(lambda text: len(text.split(\" \")) if text!=None else 0)\n",
        "# adding a new feature number of tags per question\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)\n",
        "df_no_dup.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ERFRyARo1J6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Removing question without any tags¶\n",
        "\n",
        "df_no_dup = df_no_dup[df_no_dup['tag_count']!=0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iItMHo6MU948",
        "scrolled": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# distribution of number of tags per question\n",
        "df_no_dup.tag_count.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2xMgCGUKU95C",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating a new database with no duplicates\n",
        "if not os.path.isfile('train_no_dup.db'):\n",
        "    disk_dup = create_engine(\"sqlite:///train_no_dup.db\")\n",
        "    no_dup = pd.DataFrame(df_no_dup, columns=['Title', 'Body', 'Tags'])\n",
        "    no_dup.to_sql('no_dup_train',disk_dup)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6Ou53MzeU95H",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This method seems more appropriate to work with this much data.\n",
        "#creating the connection with database file.\n",
        "if os.path.isfile('train_no_dup.db'):\n",
        "    start = datetime.now()\n",
        "    con = sqlite3.connect('train_no_dup.db')\n",
        "    tag_data = pd.read_sql_query(\"\"\"SELECT Tags FROM no_dup_train\"\"\", con)\n",
        "    #Always remember to close the database\n",
        "    con.close()\n",
        "\n",
        "    # Let's now drop unwanted column.\n",
        "    tag_data.drop(tag_data.index[0], inplace=True)\n",
        "    #Printing first 5 columns from our data frame\n",
        "    tag_data.head()\n",
        "    print(\"Time taken to run this cell :\", datetime.now() - start)\n",
        "else:\n",
        "    print(\"Please download the train.db file from drive or run the above cells to genarate train.db file\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2jZiNhBS1J7G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Data Insights </h2>"
      ]
    },
    {
      "metadata": {
        "id": "_BTSXUej1J7P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_no_dup.db.shape \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5vWjeU6P1J7Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_no_dup.db.columns.values \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9PKyBUo51J7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_no_dup.info() \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UG2sBO371J7y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_no_dup.describe() \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nNSr8SoJ1J8A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## In order to get the summary of non numerical features\n",
        "train_no_dup.describe(include=['object'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aNxNZu781J8M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#apply() --> apply function to each column\n",
        "train_no_dup.apply(np.max)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8u5ly4xD1J8d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#To check missing values\n",
        "\n",
        "sns.heatmap(train_no_dup.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NrzYk6HF1J8m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dataset has no missing values.\n",
        "\n",
        "If there were any, you would've noticed in figure represented by different colour shade on purple background.\n",
        "\n",
        "Do try it out with other dataset which has missing values,you'll see the difference."
      ]
    },
    {
      "metadata": {
        "id": "3Hp_c4kj1J8p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#To check correlation¶\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(train_no_dup.corr(),cmap='Blues',annot=False) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wN7AlnI01J8z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dark shades represents positive correlation while lighter shades represents negative correlation.\n",
        "\n",
        "If you set annot=True, you'll get values by which features are correlated to each other in grid-cells"
      ]
    },
    {
      "metadata": {
        "id": "pI4M1wJk1J81",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Quality correlation matrix\n",
        "k = 12 #number of variables for heatmap\n",
        "cols = train_no_dup.corr().nlargest(k, 'quality')['quality'].index\n",
        "cm = train_no_dup[cols].corr()\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(cm, annot=True, cmap = 'viridis')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "roKojCL01J8_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we can infer that \"density\" has strong positive correlation with \"residual sugar\" whereas it has strong negative correlation with \"alcohol\".\n",
        "\"free sulphur dioxide\" and \"citric acid\" has almost no correlation with \"quality\"\n",
        "Since correlation is zero we can infer there is no linear relationship between these two predictors.However it is safe to drop these features in case you're applying Linear Regression model to the dataset."
      ]
    },
    {
      "metadata": {
        "id": "dwIDIvc71J9G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#2-D scatter plot:\n",
        "#ALWAYS understand the axis: labels and scale.\n",
        "\n",
        "iris.plot(kind='scatter', x='sepal_length', y='sepal_width') ;\n",
        "plt.show()\n",
        "\n",
        "#cannot make much sense out it. \n",
        "#What if we color the points by thier class-label/flower-type."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AbWWj8Rv1J9P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 2-D Scatter plot with color-coding for each flower type/class.\n",
        "# Here 'sns' corresponds to seaborn. \n",
        "sns.set_style(\"whitegrid\");\n",
        "sns.FacetGrid(iris, hue=\"species\", size=4) \\\n",
        "   .map(plt.scatter, \"sepal_length\", \"sepal_width\") \\\n",
        "   .add_legend();\n",
        "plt.show();\n",
        "\n",
        "# Notice that the blue points can be easily seperated \n",
        "# from red and green by drawing a line. \n",
        "# But red and green data points cannot be easily seperated.\n",
        "# Can we draw multiple 2-D scatter plots for each combination of features?\n",
        "# How many cobinations exist? 4C2 = 6."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9AaX85XH1J9Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3D Scatter plot\n",
        "\n",
        "https://plot.ly/pandas/3d-scatter-plots/\n",
        "\n",
        "Needs a lot to mouse interaction to interpret data.\n",
        "\n",
        "What about 4-D, 5-D or n-D scatter plot?"
      ]
    },
    {
      "metadata": {
        "id": "7wzK-J-j1J9e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  (3.3) Pair-plot"
      ]
    },
    {
      "metadata": {
        "id": "UGGzS3hR1J9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# pairwise scatter plot: Pair-Plot\n",
        "# Dis-advantages: \n",
        "##Can be used when number of features are high.\n",
        "##Cannot visualize higher dimensional patterns in 3-D and 4-D. \n",
        "#Only possible to view 2D patterns.\n",
        "plt.close();\n",
        "sns.set_style(\"whitegrid\");\n",
        "sns.pairplot(iris, hue=\"species\", size=3);\n",
        "plt.show()\n",
        "# NOTE: the diagnol elements are PDFs for each feature. PDFs are expalined below."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ymsVREcm1J97",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "1. petal_length and petal_width are the most useful features to identify various flower types.\n",
        "2. While Setosa can be easily identified (linearly seperable), Virnica and Versicolor have some overlap (almost linearly seperable).\n",
        "3. We can find \"lines\" and \"if-else\" conditions to build a simple model to classify the flower types."
      ]
    },
    {
      "metadata": {
        "id": "G3JPmDZo1J-A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# (3.4) Histogram, PDF, CDF"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "9CXa_6D91J-E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# What about 1-D scatter plot using just one feature?\n",
        "#1-D scatter plot of petal-length\n",
        "import numpy as np\n",
        "iris_setosa = iris.loc[iris[\"species\"] == \"setosa\"];\n",
        "iris_virginica = iris.loc[iris[\"species\"] == \"virginica\"];\n",
        "iris_versicolor = iris.loc[iris[\"species\"] == \"versicolor\"];\n",
        "#print(iris_setosa[\"petal_length\"])\n",
        "plt.plot(iris_setosa[\"petal_length\"], np.zeros_like(iris_setosa['petal_length']), 'o')\n",
        "plt.plot(iris_versicolor[\"petal_length\"], np.zeros_like(iris_versicolor['petal_length']), 'o')\n",
        "plt.plot(iris_virginica[\"petal_length\"], np.zeros_like(iris_virginica['petal_length']), 'o')\n",
        "\n",
        "plt.show()\n",
        "#Disadvantages of 1-D scatter plot: Very hard to make sense as points\n",
        "#are overlapping a lot.\n",
        "#Are there better ways of visualizing 1-D scatter plots?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "YnRb-Ky81J-s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sns.FacetGrid(iris, hue=\"species\", size=5) \\\n",
        "   .map(sns.distplot, \"petal_length\") \\\n",
        "   .add_legend();\n",
        "plt.show();\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DRtKcfQ01J-5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sns.FacetGrid(iris, hue=\"species\", size=5) \\\n",
        "   .map(sns.distplot, \"petal_width\") \\\n",
        "   .add_legend();\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "10Zujpon1J_G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sns.FacetGrid(iris, hue=\"species\", size=5) \\\n",
        "   .map(sns.distplot, \"sepal_length\") \\\n",
        "   .add_legend();\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tftFTR7F1J_V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sns.FacetGrid(iris, hue=\"species\", size=5) \\\n",
        "   .map(sns.distplot, \"sepal_width\") \\\n",
        "   .add_legend();\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aStcpWVn1J_h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Histograms and Probability Density Functions (PDF) using KDE\n",
        "# How to compute PDFs using counts/frequencies of data points in each window.\n",
        "# How window width effects the PDF plot.\n",
        "\n",
        "\n",
        "# Interpreting a PDF:\n",
        "## why is it called a density plot?\n",
        "## Why is it called a probability plot?\n",
        "## for each value of petal_length, what does the value on y-axis mean?\n",
        "# Notice that we can write a simple if..else condition as if(petal_length) < 2.5 then flower type is setosa.\n",
        "# Using just one feature, we can build a simple \"model\" suing if..else... statements.\n",
        "\n",
        "# Disadv of PDF: Can we say what percentage of versicolor points have a petal_length of less than 5?\n",
        "\n",
        "# Do some of these plots look like a bell-curve you studied in under-grad?\n",
        "# Gaussian/Normal distribution.\n",
        "# What is \"normal\" about normal distribution?\n",
        "# e.g: Hieghts of male students in a class.\n",
        "# One of the most frequent distributions in nature.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "a8jgEcrq1J_q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Need for Cumulative Distribution Function (CDF)\n",
        "# We can visually see what percentage of versicolor flowers have a \n",
        "# petal_length of less than 5?\n",
        "# How to construct a CDF?\n",
        "# How to read a CDF?\n",
        "\n",
        "#Plot CDF of petal_length\n",
        "\n",
        "counts, bin_edges = np.histogram(iris_setosa['petal_length'], bins=10, \n",
        "                                 density = True)\n",
        "pdf = counts/(sum(counts))\n",
        "print(pdf);\n",
        "print(bin_edges);\n",
        "cdf = np.cumsum(pdf)\n",
        "plt.plot(bin_edges[1:],pdf);\n",
        "plt.plot(bin_edges[1:], cdf)\n",
        "\n",
        "\n",
        "counts, bin_edges = np.histogram(iris_setosa['petal_length'], bins=20, \n",
        "                                 density = True)\n",
        "pdf = counts/(sum(counts))\n",
        "plt.plot(bin_edges[1:],pdf);\n",
        "\n",
        "plt.show();\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GuILOhF31J_9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Need for Cumulative Distribution Function (CDF)\n",
        "# We can visually see what percentage of versicolor flowers have a \n",
        "# petal_length of less than 1.6?\n",
        "# How to construct a CDF?\n",
        "# How to read a CDF?\n",
        "\n",
        "#Plot CDF of petal_length\n",
        "\n",
        "counts, bin_edges = np.histogram(iris_setosa['petal_length'], bins=10, \n",
        "                                 density = True)\n",
        "pdf = counts/(sum(counts))\n",
        "print(pdf);\n",
        "print(bin_edges)\n",
        "\n",
        "#compute CDF\n",
        "cdf = np.cumsum(pdf)\n",
        "plt.plot(bin_edges[1:],pdf)\n",
        "plt.plot(bin_edges[1:], cdf)\n",
        "\n",
        "\n",
        "\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bf9uMzfW1KAT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plots of CDF of petal_length for various types of flowers.\n",
        "\n",
        "# Misclassification error if you use petal_length only.\n",
        "\n",
        "counts, bin_edges = np.histogram(iris_setosa['petal_length'], bins=10, \n",
        "                                 density = True)\n",
        "pdf = counts/(sum(counts))\n",
        "print(pdf);\n",
        "print(bin_edges)\n",
        "cdf = np.cumsum(pdf)\n",
        "plt.plot(bin_edges[1:],pdf)\n",
        "plt.plot(bin_edges[1:], cdf)\n",
        "\n",
        "\n",
        "# virginica\n",
        "counts, bin_edges = np.histogram(iris_virginica['petal_length'], bins=10, \n",
        "                                 density = True)\n",
        "pdf = counts/(sum(counts))\n",
        "print(pdf);\n",
        "print(bin_edges)\n",
        "cdf = np.cumsum(pdf)\n",
        "plt.plot(bin_edges[1:],pdf)\n",
        "plt.plot(bin_edges[1:], cdf)\n",
        "\n",
        "\n",
        "#versicolor\n",
        "counts, bin_edges = np.histogram(iris_versicolor['petal_length'], bins=10, \n",
        "                                 density = True)\n",
        "pdf = counts/(sum(counts))\n",
        "print(pdf);\n",
        "print(bin_edges)\n",
        "cdf = np.cumsum(pdf)\n",
        "plt.plot(bin_edges[1:],pdf)\n",
        "plt.plot(bin_edges[1:], cdf)\n",
        "\n",
        "\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HpVVF8N1KAg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# (3.5) Mean, Variance and Std-dev"
      ]
    },
    {
      "metadata": {
        "id": "4EEDtOG41KAj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Mean, Variance, Std-deviation,  \n",
        "print(\"Means:\")\n",
        "print(np.mean(iris_setosa[\"petal_length\"]))\n",
        "#Mean with an outlier.\n",
        "print(np.mean(np.append(iris_setosa[\"petal_length\"],50)));\n",
        "print(np.mean(iris_virginica[\"petal_length\"]))\n",
        "print(np.mean(iris_versicolor[\"petal_length\"]))\n",
        "\n",
        "print(\"\\nStd-dev:\");\n",
        "print(np.std(iris_setosa[\"petal_length\"]))\n",
        "print(np.std(iris_virginica[\"petal_length\"]))\n",
        "print(np.std(iris_versicolor[\"petal_length\"]))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ncR8F7px1KA0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# (3.6) Median, Percentile, Quantile, IQR, MAD"
      ]
    },
    {
      "metadata": {
        "id": "YGWA3ola1KA6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Median, Quantiles, Percentiles, IQR.\n",
        "print(\"\\nMedians:\")\n",
        "print(np.median(iris_setosa[\"petal_length\"]))\n",
        "#Median with an outlier\n",
        "print(np.median(np.append(iris_setosa[\"petal_length\"],50)));\n",
        "print(np.median(iris_virginica[\"petal_length\"]))\n",
        "print(np.median(iris_versicolor[\"petal_length\"]))\n",
        "\n",
        "\n",
        "print(\"\\nQuantiles:\")\n",
        "print(np.percentile(iris_setosa[\"petal_length\"],np.arange(0, 100, 25)))\n",
        "print(np.percentile(iris_virginica[\"petal_length\"],np.arange(0, 100, 25)))\n",
        "print(np.percentile(iris_versicolor[\"petal_length\"], np.arange(0, 100, 25)))\n",
        "\n",
        "print(\"\\n90th Percentiles:\")\n",
        "print(np.percentile(iris_setosa[\"petal_length\"],90))\n",
        "print(np.percentile(iris_virginica[\"petal_length\"],90))\n",
        "print(np.percentile(iris_versicolor[\"petal_length\"], 90))\n",
        "\n",
        "from statsmodels import robust\n",
        "print (\"\\nMedian Absolute Deviation\")\n",
        "print(robust.mad(iris_setosa[\"petal_length\"]))\n",
        "print(robust.mad(iris_virginica[\"petal_length\"]))\n",
        "print(robust.mad(iris_versicolor[\"petal_length\"]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fZiONkpF1KBK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# (3.7) Box plot and Whiskers"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "9NOeQkkr1KBN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Box-plot with whiskers: another method of visualizing the  1-D scatter plot more intuitivey.\n",
        "# The Concept of median, percentile, quantile.\n",
        "# How to draw the box in the box-plot?\n",
        "# How to draw whiskers: [no standard way] Could use min and max or use other complex statistical techniques.\n",
        "# IQR like idea.\n",
        "\n",
        "#NOTE: IN the plot below, a technique call inter-quartile range is used in plotting the whiskers. \n",
        "#Whiskers in the plot below donot correposnd to the min and max values.\n",
        "\n",
        "#Box-plot can be visualized as a PDF on the side-ways.\n",
        "\n",
        "sns.boxplot(x='species',y='petal_length', data=iris)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RfireZUQ1KBe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# (3.8) Violin plots"
      ]
    },
    {
      "metadata": {
        "id": "04s3O7o71KBh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A violin plot combines the benefits of the previous two plots \n",
        "#and simplifies them\n",
        "\n",
        "# Denser regions of the data are fatter, and sparser ones thinner \n",
        "#in a violin plot\n",
        "\n",
        "sns.violinplot(x=\"species\", y=\"petal_length\", data=iris, size=8)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-lWVZcSh1KBz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "41kWJsDu1KCH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> To check Outliers </h2>"
      ]
    },
    {
      "metadata": {
        "id": "mjojPEa01KCJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l = train_no_dup.columns.values\n",
        "number_of_columns=12\n",
        "number_of_rows = len(l)-1/number_of_columns\n",
        "plt.figure(figsize=(number_of_columns,5*number_of_rows))\n",
        "for i in range(0,len(l)):\n",
        "    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n",
        "    sns.set_style('whitegrid')\n",
        "    sns.boxplot(train_no_dup[l[i]],color='green',orient='v')\n",
        "    plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8tZkh3TF1KCS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> To check distribution-Skewness </h2>\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "HHz1U2Hl1KCT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2*number_of_columns,5*number_of_rows))\n",
        "for i in range(0,len(l)):\n",
        "    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n",
        "    sns.distplot(df[l[i]],kde=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "soyI01yc1KCg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plotting \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# Bar Graph --> count plot\n",
        "sns.countplot(x='',hue='',data=df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hwZVL3doU95O"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 3.2 Analysis of Tags </h2>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zYs5peW8U95P"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 3.2.1 Total number of unique tags </h3>"
      ]
    },
    {
      "metadata": {
        "id": "Hkj2d3pt1KCq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# removing 1st row its extra\n",
        "df_no_dup=df_no_dup.drop(df_no_dup.index[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fJECqLEJ1KCz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#by default 'split()' will tokenize each tag using space.\n",
        "vectorizer = CountVectorizer(tokenizer = lambda x: x.split())\n",
        "# fit_transform() does two functions: First, it fits the model\n",
        "# and learns the vocabulary; second, it transforms our training data\n",
        "# into feature vectors. The input to fit_transform should be a list of strings.\n",
        "tag_dtm = vectorizer.fit_transform(tag_data['Tags'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Oz5N0GH0U95V",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Number of data points :\", tag_dtm.shape[0])\n",
        "print(\"Number of unique tags :\", tag_dtm.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Otn6CUuQU95b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#'get_feature_name()' gives us the vocabulary.\n",
        "tags = vectorizer.get_feature_names()\n",
        "#Lets look at the tags we have.\n",
        "print(\"Some of the tags we have :\", tags[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NQa3ETSeU95g"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 3.2.3 Number of times a tag appeared </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "beThyuyqU95h",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/15115765/how-to-access-sparse-matrix-elements\n",
        "#Lets now store the document term matrix in a dictionary.\n",
        "freqs = tag_dtm.sum(axis=0).A1\n",
        "result = dict(zip(tags, freqs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6ALwSSx9U95m",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Saving this dictionary to csv files.\n",
        "if not os.path.isfile('tag_counts_dict_dtm.csv'):\n",
        "    with open('tag_counts_dict_dtm.csv', 'w') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        for key, value in result.items():\n",
        "            writer.writerow([key, value])\n",
        "tag_df = pd.read_csv(\"tag_counts_dict_dtm.csv\", names=['Tags', 'Counts'])\n",
        "tag_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mzmS8yiNU95t",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tag_df_sorted = tag_df.sort_values(['Counts'], ascending=False)\n",
        "tag_counts = tag_df_sorted['Counts'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6mcj55FIU95z",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(tag_counts)\n",
        "plt.title(\"Distribution of number of times tag appeared questions\")\n",
        "plt.grid()\n",
        "plt.xlabel(\"Tag number\")\n",
        "plt.ylabel(\"Number of times tag appeared\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UzTqln6XU955",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(tag_counts[0:10000])\n",
        "plt.title('first 10k tags: Distribution of number of times tag appeared questions')\n",
        "plt.grid()\n",
        "plt.xlabel(\"Tag number\")\n",
        "plt.ylabel(\"Number of times tag appeared\")\n",
        "plt.show()\n",
        "print(len(tag_counts[0:10000:25]), tag_counts[0:10000:25])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ntm8E_K9U95-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(tag_counts[0:1000])\n",
        "plt.title('first 1k tags: Distribution of number of times tag appeared questions')\n",
        "plt.grid()\n",
        "plt.xlabel(\"Tag number\")\n",
        "plt.ylabel(\"Number of times tag appeared\")\n",
        "plt.show()\n",
        "print(len(tag_counts[0:1000:5]), tag_counts[0:1000:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-SRUeKuWU96I",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(tag_counts[0:500])\n",
        "plt.title('first 500 tags: Distribution of number of times tag appeared questions')\n",
        "plt.grid()\n",
        "plt.xlabel(\"Tag number\")\n",
        "plt.ylabel(\"Number of times tag appeared\")\n",
        "plt.show()\n",
        "print(len(tag_counts[0:500:5]), tag_counts[0:500:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CCOE0Nu4U96N",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(tag_counts[0:100], c='b')\n",
        "plt.scatter(x=list(range(0,100,5)), y=tag_counts[0:100:5], c='orange', label=\"quantiles with 0.05 intervals\")\n",
        "# quantiles with 0.25 difference\n",
        "plt.scatter(x=list(range(0,100,25)), y=tag_counts[0:100:25], c='m', label = \"quantiles with 0.25 intervals\")\n",
        "\n",
        "for x,y in zip(list(range(0,100,25)), tag_counts[0:100:25]):\n",
        "    plt.annotate(s=\"({} , {})\".format(x,y), xy=(x,y), xytext=(x-0.05, y+500))\n",
        "\n",
        "plt.title('first 100 tags: Distribution of number of times tag appeared questions')\n",
        "plt.grid()\n",
        "plt.xlabel(\"Tag number\")\n",
        "plt.ylabel(\"Number of times tag appeared\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print(len(tag_counts[0:100:5]), tag_counts[0:100:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MnwXytypU96R",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Store tags greater than 10K in one list\n",
        "lst_tags_gt_10k = tag_df[tag_df.Counts>10000].Tags\n",
        "#Print the length of the list\n",
        "print ('{} Tags are used more than 10000 times'.format(len(lst_tags_gt_10k)))\n",
        "# Store tags greater than 100K in one list\n",
        "lst_tags_gt_100k = tag_df[tag_df.Counts>100000].Tags\n",
        "#Print the length of the list.\n",
        "print ('{} Tags are used more than 100000 times'.format(len(lst_tags_gt_100k)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oOXPF7q1U96W"
      },
      "cell_type": "markdown",
      "source": [
        "<b>Observations:</b><br />\n",
        "1. There are total 153 tags which are used more than 10000 times.\n",
        "2. 14 tags are used more than 100000 times.\n",
        "3. Most frequent tag (i.e. c#) is used 331505 times.\n",
        "4. Since some tags occur much more frequenctly than others, Micro-averaged F1-score is the appropriate metric for this probelm."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zb-Tg_sgU96a"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 3.2.4 Tags Per Question </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bsPStbjGU96g",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Storing the count of tag in each question in list 'tag_count'\n",
        "tag_quest_count = tag_dtm.sum(axis=1).tolist()\n",
        "#Converting each value in the 'tag_quest_count' to integer.\n",
        "tag_quest_count=[int(j) for i in tag_quest_count for j in i]\n",
        "print ('We have total {} datapoints.'.format(len(tag_quest_count)))\n",
        "\n",
        "print(tag_quest_count[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DLbm7crfU96n",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print( \"Maximum number of tags per question: %d\"%max(tag_quest_count))\n",
        "print( \"Minimum number of tags per question: %d\"%min(tag_quest_count))\n",
        "print( \"Avg. number of tags per question: %f\"% ((sum(tag_quest_count)*1.0)/len(tag_quest_count)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mb1vdd8KU96x",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sns.countplot(tag_quest_count, palette='gist_rainbow')\n",
        "plt.title(\"Number of tags in the questions \")\n",
        "plt.xlabel(\"Number of Tags\")\n",
        "plt.ylabel(\"Number of questions\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0RsUcQkNU963"
      },
      "cell_type": "markdown",
      "source": [
        "<b>Observations:</b><br />\n",
        "1. Maximum number of tags per question: 5\n",
        "2. Minimum number of tags per question: 1\n",
        "3. Avg. number of tags per question: 2.899\n",
        "4. Most of the questions are having 2 or 3 tags"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7-M5-A-MU963"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>3.2.5 Most Frequent Tags </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Brokf0gSU965",
        "scrolled": false,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Ploting word cloud\n",
        "start = datetime.now()\n",
        "\n",
        "# Lets first convert the 'result' dictionary to 'list of tuples'\n",
        "tup = dict(result.items())\n",
        "#Initializing WordCloud using frequencies of tags.\n",
        "wordcloud = WordCloud(    background_color='black',\n",
        "                          width=1600,\n",
        "                          height=800,\n",
        "                    ).generate_from_frequencies(tup)\n",
        "\n",
        "fig = plt.figure(figsize=(30,20))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "fig.savefig(\"tag.png\")\n",
        "plt.show()\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_n7RaKj2U96-"
      },
      "cell_type": "markdown",
      "source": [
        "<b>Observations:</b><br />\n",
        "A look at the word cloud shows that \"c#\", \"java\", \"php\", \"asp.net\", \"javascript\", \"c++\" are some of the most frequent tags."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Hpc8IWjqU96_"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 3.2.6 The top 20 tags </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ov3WmIEHU97A",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i=np.arange(30)\n",
        "tag_df_sorted.head(30).plot(kind='bar')\n",
        "plt.title('Frequency of top 20 tags')\n",
        "plt.xticks(i, tag_df_sorted['Tags'])\n",
        "plt.xlabel('Tags')\n",
        "plt.ylabel('Counts')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rlmizz7LU97D"
      },
      "cell_type": "markdown",
      "source": [
        "<b>Observations:</b><br />\n",
        "1. Majority of the most frequent tags are programming language.\n",
        "2. C# is the top most frequent programming language.\n",
        "3. Android, IOS, Linux and windows are among the top most frequent operating systems."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "I-Z7F0_mU97F"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4. Cleaning and preprocessing of Questions </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Jvi2298wU986"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.1 Preprocessing of questions </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JNhcD1LYU987"
      },
      "cell_type": "markdown",
      "source": [
        "<ol> \n",
        "    <li> Separate Code from Body </li>\n",
        "    <li> Sampling 0.5million datapoints </li>\n",
        "    <li> Remove Spcial characters from Question title and description (not in code)</li>\n",
        "    <li> <b> Give more weightage to title : Add title three times to the question </b> </li>\n",
        "    <li> Remove stop words (Except 'C') </li>\n",
        "    <li> Remove HTML Tags </li>\n",
        "    <li> Convert all the characters into small letters </li>\n",
        "    <li> Use SnowballStemmer to stem the words </li>\n",
        "</ol>"
      ]
    },
    {
      "metadata": {
        "id": "j105RZVZ1KHn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def striphtml(data):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', str(data))\n",
        "    return cleantext\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJD55hCj1KHs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_connection(db_file):\n",
        "    \"\"\" create a database connection to the SQLite database\n",
        "        specified by db_file\n",
        "    :param db_file: database file\n",
        "    :return: Connection object or None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file)\n",
        "        return conn\n",
        "    except Error as e:\n",
        "        print(e)\n",
        " \n",
        "    return None\n",
        "\n",
        "def create_table(conn, create_table_sql):\n",
        "    \"\"\" create a table from the create_table_sql statement\n",
        "    :param conn: Connection object\n",
        "    :param create_table_sql: a CREATE TABLE statement\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        c = conn.cursor()\n",
        "        c.execute(create_table_sql)\n",
        "    except Error as e:\n",
        "        print(e)\n",
        "        \n",
        "def checkTableExists(dbcon):\n",
        "    cursr = dbcon.cursor()\n",
        "    str = \"select name from sqlite_master where type='table'\"\n",
        "    table_names = cursr.execute(str)\n",
        "    print(\"Tables in the databse:\")\n",
        "    tables =table_names.fetchall() \n",
        "    print(tables[0][0])\n",
        "    return(len(tables))\n",
        "\n",
        "def create_database_table(database, query):\n",
        "    conn = create_connection(database)\n",
        "    if conn is not None:\n",
        "        create_table(conn, query)\n",
        "        checkTableExists(conn)\n",
        "    else:\n",
        "        print(\"Error! cannot create the database connection.\")\n",
        "    conn.close()\n",
        "\n",
        "sql_create_table = \"\"\"CREATE TABLE IF NOT EXISTS QuestionsProcessed (question text NOT NULL, code text, tags text, words_pre integer, words_post integer, is_code integer);\"\"\"\n",
        "create_database_table(\"3times_weighted_Title.db\", sql_create_table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XHLunpYsU982",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://www.sqlitetutorial.net/sqlite-delete/\n",
        "# https://stackoverflow.com/questions/2279706/select-random-row-from-a-sqlite-table\n",
        "\n",
        "read_db = 'train_no_dup.db'\n",
        "write_db = 'Titlemoreweight.db'\n",
        "train_datasize = 400000\n",
        "if os.path.isfile(read_db):\n",
        "    conn_r = create_connection(read_db)\n",
        "    if conn_r is not None:\n",
        "        reader =conn_r.cursor()\n",
        "        # for selecting first 0.5M rows\n",
        "        reader.execute(\"SELECT Title, Body, Tags From no_dup_train LIMIT 500001;\")\n",
        "        # for selecting random points\n",
        "        #reader.execute(\"SELECT Title, Body, Tags From no_dup_train ORDER BY RANDOM() LIMIT 500001;\")\n",
        "\n",
        "if os.path.isfile(write_db):\n",
        "    conn_w = create_connection(write_db)\n",
        "    if conn_w is not None:\n",
        "        tables = checkTableExists(conn_w)\n",
        "        writer =conn_w.cursor()\n",
        "        if tables != 0:\n",
        "            writer.execute(\"DELETE FROM QuestionsProcessed WHERE 1\")\n",
        "            print(\"Cleared All the rows\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ifSmL0M-U98-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#http://www.bernzilla.com/2008/05/13/selecting-a-random-row-from-an-sqlite-table/\n",
        "start = datetime.now()\n",
        "preprocessed_data_list=[]\n",
        "reader.fetchone()\n",
        "questions_with_code=0\n",
        "len_pre=0\n",
        "len_post=0\n",
        "questions_proccesed = 0\n",
        "for row in reader:\n",
        "    \n",
        "    is_code = 0\n",
        "    \n",
        "    title, question, tags = row[0], row[1], str(row[2])\n",
        "    \n",
        "    if '<code>' in question:\n",
        "        questions_with_code+=1\n",
        "        is_code = 1\n",
        "    x = len(question)+len(title)\n",
        "    len_pre+=x\n",
        "    \n",
        "    code = str(re.findall(r'<code>(.*?)</code>', question, flags=re.DOTALL))\n",
        "    \n",
        "    question=re.sub('<code>(.*?)</code>', '', question, flags=re.MULTILINE|re.DOTALL)\n",
        "    question=striphtml(question.encode('utf-8'))\n",
        "    \n",
        "    title=title.encode('utf-8')\n",
        "    \n",
        "    # adding title three time to the data to increase its weight\n",
        "    # add tags string to the training data\n",
        "    \n",
        "    question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question\n",
        "    \n",
        "#     if questions_proccesed<=train_datasize:\n",
        "#         question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question+\" \"+str(tags)\n",
        "#     else:\n",
        "#         question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question\n",
        "\n",
        "    question=re.sub(r'[^A-Za-z0-9#+.\\-]+',' ',question)\n",
        "    words=word_tokenize(str(question.lower()))\n",
        "    \n",
        "    #Removing all single letter and and stopwords from question exceptt for the letter 'c'\n",
        "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
        "    \n",
        "    len_post+=len(question)\n",
        "    tup = (question,code,tags,x,len(question),is_code)\n",
        "    questions_proccesed += 1\n",
        "    writer.execute(\"insert into QuestionsProcessed(question,code,tags,words_pre,words_post,is_code) values (?,?,?,?,?,?)\",tup)\n",
        "    if (questions_proccesed%50000==0):\n",
        "        print(\"number of questions completed=\",questions_proccesed)\n",
        "\n",
        "no_dup_avg_len_pre=(len_pre*1.0)/questions_proccesed\n",
        "no_dup_avg_len_post=(len_post*1.0)/questions_proccesed\n",
        "\n",
        "print( \"Avg. length of questions(Title+Body) before processing: %d\"%no_dup_avg_len_pre)\n",
        "print( \"Avg. length of questions(Title+Body) after processing: %d\"%no_dup_avg_len_post)\n",
        "print (\"Percent of questions containing code: %d\"%((questions_with_code*100.0)/questions_proccesed))\n",
        "\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "x54WQvZAU99B",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# never forget to close the conections or else we will end up with database locks\n",
        "conn_r.commit()\n",
        "conn_w.commit()\n",
        "conn_r.close()\n",
        "conn_w.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gpN1ZM2bU99F"
      },
      "cell_type": "markdown",
      "source": [
        "__ Sample quesitons after preprocessing of data __"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ytEecnCtU99H",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if os.path.isfile(write_db):\n",
        "    conn_r = create_connection(write_db)\n",
        "    if conn_r is not None:\n",
        "        reader =conn_r.cursor()\n",
        "        reader.execute(\"SELECT question From QuestionsProcessed LIMIT 10\")\n",
        "        print(\"Questions after preprocessed\")\n",
        "        print('='*100)\n",
        "        reader.fetchone()\n",
        "        for row in reader:\n",
        "            print(row)\n",
        "            print('-'*100)\n",
        "conn_r.commit()\n",
        "conn_r.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "IspyyegoU99N"
      },
      "cell_type": "markdown",
      "source": [
        "__ Saving Preprocessed data to a Database __"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "F_x-ETQJU99P",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Taking 0.5 Million entries to a dataframe.\n",
        "write_db = 'Titlemoreweight.db'\n",
        "if os.path.isfile(write_db):\n",
        "    conn_r = create_connection(write_db)\n",
        "    if conn_r is not None:\n",
        "        preprocessed_data = pd.read_sql_query(\"\"\"SELECT question, Tags FROM QuestionsProcessed\"\"\", conn_r)\n",
        "conn_r.commit()\n",
        "conn_r.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bc7hwHjBU99U",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preprocessed_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Xk9V0azqU99X",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"number of data points in sample :\", preprocessed_data.shape[0])\n",
        "print(\"number of dimensions :\", preprocessed_data.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qB0bL2drU97w"
      },
      "cell_type": "markdown",
      "source": [
        "<h1>4. Machine Learning Models </h1>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BZ3-VPbqU97w"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 4.1 Converting tags for multilabel problems </h2>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "K88oaTD9U97y"
      },
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<tr>\n",
        "<th>X</th><th>y1</th><th>y2</th><th>y3</th><th>y4</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>x1</td><td>0</td><td>1</td><td>1</td><td>0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>x1</td><td>1</td><td>0</td><td>0</td><td>0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>x1</td><td>0</td><td>1</td><td>0</td><td>0</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "D-IWSRzo1KI3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1>4. Machine Learning Models </h1>\n",
        "\n",
        "<h2> 4.1 Converting tags for multilabel problems </h2>\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<th>X</th><th>y1</th><th>y2</th><th>y3</th><th>y4</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>x1</td><td>0</td><td>1</td><td>1</td><td>0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>x1</td><td>1</td><td>0</td><td>0</td><td>0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>x1</td><td>0</td><td>1</td><td>0</td><td>0</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oUpccCSkU99Z"
      },
      "cell_type": "markdown",
      "source": [
        "__ Converting string Tags to multilable output variables __ "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SWg_g1lNU99a",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
        "multilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TalFQ0r41KJD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__ We will sample the number of tags instead considering all of them (due to limitation of computing power) __"
      ]
    },
    {
      "metadata": {
        "id": "P8ZR9izk1KJF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tags_to_choose(n):\n",
        "    t = multilabel_y.sum(axis=0).tolist()[0]\n",
        "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n",
        "    multilabel_yn=multilabel_y[:,sorted_tags_i[:n]]\n",
        "    return multilabel_yn\n",
        "\n",
        "def questions_explained_fn(n):\n",
        "    multilabel_yn = tags_to_choose(n)\n",
        "    x= multilabel_yn.sum(axis=1)\n",
        "    return (np.count_nonzero(x==0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pbtD0Hx8U99c"
      },
      "cell_type": "markdown",
      "source": [
        "__ Selecting 500 Tags __"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "h_nMDxAIU99d",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions_explained = []\n",
        "total_tags=multilabel_y.shape[1]\n",
        "total_qs=preprocessed_data.shape[0]\n",
        "for i in range(500, total_tags, 100):\n",
        "    questions_explained.append(np.round(((total_qs-questions_explained_fn(i))/total_qs)*100,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fggMk2IJU99f",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(questions_explained)\n",
        "xlabel = list(500+np.array(range(-50,450,50))*50)\n",
        "ax.set_xticklabels(xlabel)\n",
        "plt.xlabel(\"Number of tags\")\n",
        "plt.ylabel(\"Number Questions coverd partially\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "# you can choose any number of tags based on your computing power, minimun is 500(it covers 90% of the tags)\n",
        "print(\"with \",5500,\"tags we are covering \",questions_explained[50],\"% of questions\")\n",
        "print(\"with \",500,\"tags we are covering \",questions_explained[0],\"% of questions\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VuJzfmNrU99i",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# we will be taking 500 tags\n",
        "multilabel_yx = tags_to_choose(500)\n",
        "print(\"number of questions that are not covered :\", questions_explained_fn(500),\"out of \", total_qs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TYoDrVsP1KJh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Number of tags in sample :\", multilabel_y.shape[1])\n",
        "print(\"number of tags taken :\", multilabel_yx.shape[1],\"(\",(multilabel_yx.shape[1]/multilabel_y.shape[1])*100,\"%)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "IxH1ggjxU98R"
      },
      "cell_type": "markdown",
      "source": [
        "__ We consider top 15% tags which covers  99% of the questions __"
      ]
    },
    {
      "metadata": {
        "id": "agDh-YNA1KJo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preprocessed_data.descibe()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJYgaMRI1KJs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here as you can notice mean value is less than median value of each column which is represented by 50%(50th percentile) in index column.\n",
        "\n",
        "There is notably a large difference between 75th %tile and max values of predictors “residual sugar”,”free sulfur dioxide”,”total sulfur dioxide”.\n",
        "\n",
        "Thus observations 1 and 2 suggests that there are extreme values-Outliers in our data set."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "stMfn7tMU98U"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>4.2 Split the data into test and train (80:20) </h2>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WsduwXTeU99k",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "total_size=preprocessed_data.shape[0]\n",
        "train_size=int(0.80*total_size)\n",
        "\n",
        "x_train=preprocessed_data.head(train_datasize)\n",
        "x_test=preprocessed_data.tail(preprocessed_data.shape[0] - 400000)\n",
        "\n",
        "y_train = multilabel_yx[0:train_datasize,:]\n",
        "y_test = multilabel_yx[train_datasize:preprocessed_data.shape[0],:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iZZDSH_VU99m",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Number of data points in train data :\", y_train.shape)\n",
        "print(\"Number of data points in test data :\", y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bERnbOY31KKA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# it returns a dict, keys as class labels and values as the number of data points in that class\n",
        "train_class_distribution = y_train['multilabel_yx'].value_counts().sortlevel()\n",
        "test_class_distribution = y_test['multilabel_yx'].value_counts().sortlevel()\n",
        "\n",
        "my_colors = 'rgbkymc'\n",
        "train_class_distribution.plot(kind='bar')\n",
        "plt.xlabel('tags')\n",
        "plt.ylabel('Data points per Class')\n",
        "plt.title('Distribution of yi in train data')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n",
        "# -(train_class_distribution.values): the minus sign will give us in decreasing order\n",
        "sorted_yi = np.argsort(-train_class_distribution.values)\n",
        "for i in sorted_yi:\n",
        "    print('Number of data points in class', i+1, ':',train_class_distribution.values[i], '(', np.round((train_class_distribution.values[i]/train_df.shape[0]*100), 3), '%)')\n",
        "\n",
        "    \n",
        "print('-'*80)\n",
        "my_colors = 'rgbkymc'\n",
        "test_class_distribution.plot(kind='bar')\n",
        "plt.xlabel('tags')\n",
        "plt.ylabel('Data points per Class')\n",
        "plt.title('Distribution of yi in test data')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n",
        "# -(train_class_distribution.values): the minus sign will give us in decreasing order\n",
        "sorted_yi = np.argsort(-test_class_distribution.values)\n",
        "for i in sorted_yi:\n",
        "    print('Number of data points in class', i+1, ':',test_class_distribution.values[i], '(', np.round((test_class_distribution.values[i]/test_df.shape[0]*100), 3), '%)')\n",
        "\n",
        "print('-'*80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gDJ2PvnzU99o"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.5.2 Featurizing data with BOW vectorizer upto 4 grams and compute the micro f1 score with Logistic regression(OvR) </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "530e8tW9U99o",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start = datetime.now()\n",
        "vectorizer = CountVectorizer(min_df=0.00009, max_features=200000,tokenizer = lambda x: x.split(), ngram_range=(1,4))\n",
        "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
        "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "r9iDfzXIU99t",
        "scrolled": false,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
        "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Kmmnoy4XU99v"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.5.3 Applying Logistic Regression with OneVsRest Classifier </h3>"
      ]
    },
    {
      "metadata": {
        "id": "91vnJ5U01KLM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "import pickle\n",
        "start = datetime.now()\n",
        "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'))\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'Applying Logistic Regression with OneVsRest Classifier.sav'\n",
        "pickle.dump(classifier, open(filename, 'wb'))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ]
    },
    {
      "metadata": {
        "id": "XDOx-FT31KLO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# save the model to disk\n",
        "filename = 'Applying Logistic Regression with OneVsRest Classifier.sav'\n",
        "pickle.dump(classifier, open(filename, 'wb'))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)\n",
        "\n",
        "# some time later...\n",
        " \n",
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j3H9lFiS1KLY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = loaded_model.predict (x_test_multilabel)\n",
        "\n",
        "\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
        "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
        "\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='micro')\n",
        "recall = recall_score(y_test, predictions, average='micro')\n",
        "f1 = f1_score(y_test, predictions, average='micro')\n",
        " \n",
        "print(\"Micro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='macro')\n",
        "recall = recall_score(y_test, predictions, average='macro')\n",
        "f1 = f1_score(y_test, predictions, average='macro')\n",
        " \n",
        "print(\"Macro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "print (metrics.classification_report(y_test, predictions))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vlFEQmsI1KLl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.5.3 Applying Logistic Regression with OneVsRest Classifier Hyper Parameter Tunning</h3>"
      ]
    },
    {
      "metadata": {
        "id": "iR8-WdHi1KLp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# applying grid search to find best c \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "start = datetime.now()\n",
        "tuned_parameters = [{'estimator__alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01]}]\n",
        "    \n",
        "model = GridSearchCV(OneVsRestClassifier(SGDClassifier(loss='log', penalty='l1')), tuned_parameters, scoring = 'f1_micro', cv = 2)\n",
        "model.fit(x_train_multilabel, y_train)\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'Applying Logistic Regression with OneVsRest Classifier Hyper Parameter Tunning.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wV8sHA2o1KL6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# some time later...\n",
        " \n",
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "print(loaded_model.best_estimator_)\n",
        "a = loaded_model.best_params_\n",
        "optimal_alpha = a.get('estimator__alpha')\n",
        "print(optimal_alpha)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JiPaVAML1KMC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results = loaded_model.cv_results_\n",
        "results['mean_test_score']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R2xu8zTu1KML",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "C=0.000001, 0.00001, 0.0001, 0.001, 0.01\n",
        "plt.plot(C,results['mean_test_score'],marker='o')\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('f1score')\n",
        "plt.title(\"F1score vs hyperparameter alpha\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zT1Zq6Ec1KMR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start = datetime.now()\n",
        "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=optimal_alpha, penalty='l1'))\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "# save the model to disk\n",
        "filename = 'Applying Logistic Regression with OneVsRest Classifier Hyper Parameter Tunning with alpha.sav'\n",
        "pickle.dump(classifier, open(filename, 'wb'))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nsumHOW51KMf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TPvcyjF91KMk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = loaded_model.predict (x_test_multilabel)\n",
        "\n",
        "\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
        "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
        "\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='micro')\n",
        "recall = recall_score(y_test, predictions, average='micro')\n",
        "f1 = f1_score(y_test, predictions, average='micro')\n",
        " \n",
        "print(\"Micro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='macro')\n",
        "recall = recall_score(y_test, predictions, average='macro')\n",
        "f1 = f1_score(y_test, predictions, average='macro')\n",
        " \n",
        "print(\"Macro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "print (metrics.classification_report(y_test, predictions))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NugS1Vyb1KMq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.5.3 Applying Linear SVM with OneVsRest Classifier </h3>"
      ]
    },
    {
      "metadata": {
        "id": "ehUaPBww1KMr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start = datetime.now()\n",
        "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.00001, penalty='l1'))\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "# save the model to disk\n",
        "filename = 'Applying Linear SVM with OneVsRest Classifier.sav'\n",
        "pickle.dump(classifier, open(filename, 'wb'))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QMX9iDAu1KMw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PdlRzFuZ1KM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = loaded_model.predict (x_test_multilabel)\n",
        "\n",
        "\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
        "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
        "\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='micro')\n",
        "recall = recall_score(y_test, predictions, average='micro')\n",
        "f1 = f1_score(y_test, predictions, average='micro')\n",
        " \n",
        "print(\"Micro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='macro')\n",
        "recall = recall_score(y_test, predictions, average='macro')\n",
        "f1 = f1_score(y_test, predictions, average='macro')\n",
        " \n",
        "print(\"Macro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "print (metrics.classification_report(y_test, predictions))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AruqYmVe1KNE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> 4.5.3 Applying Linear SVM with OneVsRest Classifier Hyper Parameter Tunning</h3>"
      ]
    },
    {
      "metadata": {
        "id": "LvItdP7p1KNG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# applying grid search to find best c \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "start = datetime.now()\n",
        "tuned_parameters = [{'estimator__alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01]}]\n",
        "    \n",
        "model = GridSearchCV(OneVsRestClassifier(SGDClassifier(loss='hinge', penalty='l1')), tuned_parameters, scoring = 'f1_micro', cv = 2)\n",
        "model.fit(x_train_multilabel, y_train)\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'Applying Linear SVM with OneVsRest Classifier Hyper Parameter Tunning.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pAP6eFwM1KNQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# some time later...\n",
        " \n",
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "print(loaded_model.best_estimator_)\n",
        "optimal_alpha = a.get('estimator__alpha')\n",
        "print(optimal_alpha)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_F8lkFyH1KNY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results = loaded_model.cv_results_\n",
        "results['mean_test_score']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WaBSjaOg1KNd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "C=0.000001, 0.00001, 0.0001, 0.001, 0.01\n",
        "plt.plot(C,results['mean_test_score'],marker='o')\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('f1score')\n",
        "plt.title(\"F1score vs hyperparameter alpha\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NrNgB6z91KN0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start = datetime.now()\n",
        "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=optimal_alpha, penalty='l1'))\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "# save the model to disk\n",
        "filename = 'Applying Linear SVM with OneVsRest Classifier Hyper Parameter Tunning with alpha.sav'\n",
        "pickle.dump(classifier, open(filename, 'wb'))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XBRYdIui1KN7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CcZhojrL1KN_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = loaded_model.predict (x_test_multilabel)\n",
        "\n",
        "\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
        "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
        "\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='micro')\n",
        "recall = recall_score(y_test, predictions, average='micro')\n",
        "f1 = f1_score(y_test, predictions, average='micro')\n",
        " \n",
        "print(\"Micro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='macro')\n",
        "recall = recall_score(y_test, predictions, average='macro')\n",
        "f1 = f1_score(y_test, predictions, average='macro')\n",
        " \n",
        "print(\"Macro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "print (metrics.classification_report(y_test, predictions))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)## Conclusion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QkaNTpcQ1KOG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "vSQfbaJ11KOI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "x = PrettyTable()\n",
        "\n",
        "x.field_names = [\"Classification model\",\"Regularization\",\"Hyperparameter\", \"Accuracy\",\"F1 micro\", \"F1 macro\"]\n",
        "\n",
        "x.add_row([\"Logistic Regression\", \"L1\",0.00001,0.17916,0.4056, 0.2579])\n",
        "x.add_row([\"Logistic Regression with Hyperparameter\", \"L1\",0.001,0.17854,0.4029, 0.2593])\n",
        "x.add_row([\"Linear SVM\", \"L1\",0.00001,0.10948, 0.3600, 0.2679])\n",
        "x.add_row([\"Linear SVM with Hyperparameter\", \"L1\", 0.001,0.17916,0.4056, 0.2579])\n",
        "\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o2QPEhOC1KOP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Steps Involved:-\n",
        "\n",
        "1) Connecting SQL file\n",
        "\n",
        "2) Reading Data\n",
        "\n",
        "3) Preprocessing of Tags\n",
        "\n",
        "4) Spliting data into train and test based on time (80:20)\n",
        "\n",
        "5) Distribution of y_i's in Train, Test \n",
        "\n",
        "6) Applying Machine learning Algorithms  Logistice Regression and Linear SVM \n",
        "\n",
        "7) Hyper Tunning Model\n",
        "\n",
        "8) calculating Accuracy,Precision Score,Recall Score,Classification Report\n",
        "\n",
        "11) Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "JSJTyglf1KOQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here i skipped MSE vs alpha graph because its taking lot of time i had tried and waited 7hours so i skipped here"
      ]
    },
    {
      "metadata": {
        "id": "2ROEpOuw1KOS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}